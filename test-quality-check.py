#!/usr/bin/env python3
"""
LLMChatMod Function Calling æµ‹è¯•è´¨é‡æ£€æŸ¥
æ·±å…¥åˆ†ææµ‹è¯•ä»£ç çš„é€»è¾‘æ­£ç¡®æ€§å’Œæœ€ä½³å®è·µ
"""

import os
import re
from pathlib import Path

class TestQualityChecker:
    def __init__(self):
        self.test_dir = Path("src/test/java/com/riceawa/llm")
        self.issues = []
        self.recommendations = []
        self.quality_score = 0
        self.max_score = 0
    
    def check_all_tests(self):
        """æ£€æŸ¥æ‰€æœ‰æµ‹è¯•çš„è´¨é‡"""
        print("ğŸ” å¼€å§‹è¿›è¡Œæµ‹è¯•è´¨é‡æ£€æŸ¥...")
        print("=" * 60)
        
        # æ£€æŸ¥å„ä¸ªæµ‹è¯•æ–‡ä»¶
        self.check_test_file("core/LLMConfigTest.java", "LLMConfig")
        self.check_test_file("core/LLMMessageTest.java", "LLMMessage")
        self.check_test_file("function/FunctionRegistryTest.java", "FunctionRegistry")
        self.check_test_file("function/impl/WorldInfoFunctionTest.java", "WorldInfoFunction")
        self.check_test_file("service/OpenAIServiceTest.java", "OpenAIService")
        self.check_test_file("integration/FunctionCallingIntegrationTest.java", "Integration")
        
        # æ£€æŸ¥æµ‹è¯•å¥—ä»¶
        self.check_test_suite()
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        self.generate_quality_report()
    
    def check_test_file(self, file_path, component_name):
        """æ£€æŸ¥å•ä¸ªæµ‹è¯•æ–‡ä»¶çš„è´¨é‡"""
        print(f"\nğŸ§ª æ£€æŸ¥ {component_name} æµ‹è¯•...")
        
        full_path = self.test_dir / file_path
        if not full_path.exists():
            self.issues.append(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥æµ‹è¯•ç»“æ„
            self.check_test_structure(content, component_name)
            
            # æ£€æŸ¥æµ‹è¯•æ–¹æ³•è´¨é‡
            self.check_test_methods(content, component_name)
            
            # æ£€æŸ¥Mockä½¿ç”¨
            self.check_mock_usage(content, component_name)
            
            # æ£€æŸ¥æ–­è¨€è´¨é‡
            self.check_assertions(content, component_name)
            
        except Exception as e:
            self.issues.append(f"è¯»å–æµ‹è¯•æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    def check_test_structure(self, content, component_name):
        """æ£€æŸ¥æµ‹è¯•ç»“æ„"""
        self.max_score += 10
        score = 0
        
        # æ£€æŸ¥åŒ…å£°æ˜
        if re.search(r'package\s+com\.riceawa\.llm', content):
            score += 2
            print("  âœ… åŒ…å£°æ˜æ­£ç¡®")
        else:
            self.issues.append(f"{component_name}: åŒ…å£°æ˜ç¼ºå¤±æˆ–é”™è¯¯")
        
        # æ£€æŸ¥å¯¼å…¥è¯­å¥
        if 'import org.junit.jupiter.api.Test' in content:
            score += 2
            print("  âœ… JUnit 5 å¯¼å…¥æ­£ç¡®")
        else:
            self.issues.append(f"{component_name}: JUnit 5 å¯¼å…¥ç¼ºå¤±")
        
        # æ£€æŸ¥é™æ€å¯¼å…¥
        if 'import static org.junit.jupiter.api.Assertions.*' in content:
            score += 2
            print("  âœ… æ–­è¨€é™æ€å¯¼å…¥æ­£ç¡®")
        else:
            self.issues.append(f"{component_name}: æ–­è¨€é™æ€å¯¼å…¥ç¼ºå¤±")
        
        # æ£€æŸ¥ç±»å‘½å
        if re.search(r'class\s+\w+Test\s*{', content):
            score += 2
            print("  âœ… æµ‹è¯•ç±»å‘½åç¬¦åˆè§„èŒƒ")
        else:
            self.issues.append(f"{component_name}: æµ‹è¯•ç±»å‘½åä¸ç¬¦åˆè§„èŒƒ")
        
        # æ£€æŸ¥@ExtendWithæ³¨è§£ï¼ˆå¦‚æœä½¿ç”¨Mockï¼‰
        if '@ExtendWith(MockitoExtension.class)' in content or '@Mock' not in content:
            score += 2
            print("  âœ… Mockitoæ‰©å±•é…ç½®æ­£ç¡®")
        else:
            self.issues.append(f"{component_name}: Mockitoæ‰©å±•é…ç½®ç¼ºå¤±")
        
        self.quality_score += score
    
    def check_test_methods(self, content, component_name):
        """æ£€æŸ¥æµ‹è¯•æ–¹æ³•è´¨é‡"""
        self.max_score += 15
        score = 0
        
        # æŸ¥æ‰¾æ‰€æœ‰æµ‹è¯•æ–¹æ³•
        test_methods = re.findall(r'@Test\s+(?:void\s+)?(\w+)', content)
        
        if len(test_methods) >= 3:
            score += 5
            print(f"  âœ… æµ‹è¯•æ–¹æ³•æ•°é‡å……è¶³ ({len(test_methods)} ä¸ª)")
        else:
            self.issues.append(f"{component_name}: æµ‹è¯•æ–¹æ³•æ•°é‡ä¸è¶³")
        
        # æ£€æŸ¥æµ‹è¯•æ–¹æ³•å‘½å
        good_naming = 0
        for method in test_methods:
            if method.startswith('test') and len(method) > 10:
                good_naming += 1
        
        if good_naming >= len(test_methods) * 0.8:
            score += 5
            print("  âœ… æµ‹è¯•æ–¹æ³•å‘½åè§„èŒƒ")
        else:
            self.recommendations.append(f"{component_name}: å»ºè®®æ”¹è¿›æµ‹è¯•æ–¹æ³•å‘½å")
        
        # æ£€æŸ¥@BeforeEachå’Œ@AfterEachä½¿ç”¨
        if '@BeforeEach' in content:
            score += 3
            print("  âœ… ä½¿ç”¨äº†@BeforeEachè¿›è¡Œæµ‹è¯•å‡†å¤‡")
        
        if '@AfterEach' in content:
            score += 2
            print("  âœ… ä½¿ç”¨äº†@AfterEachè¿›è¡Œæ¸…ç†")
        
        self.quality_score += score
    
    def check_mock_usage(self, content, component_name):
        """æ£€æŸ¥Mockä½¿ç”¨è´¨é‡"""
        self.max_score += 10
        score = 0
        
        # æ£€æŸ¥Mockæ³¨è§£ä½¿ç”¨
        mock_count = len(re.findall(r'@Mock', content))
        if mock_count > 0:
            score += 3
            print(f"  âœ… æ­£ç¡®ä½¿ç”¨Mockå¯¹è±¡ ({mock_count} ä¸ª)")
            
            # æ£€æŸ¥Mockå¯¹è±¡åˆå§‹åŒ–
            if 'when(' in content:
                score += 3
                print("  âœ… Mockå¯¹è±¡è¡Œä¸ºé…ç½®æ­£ç¡®")
            else:
                self.issues.append(f"{component_name}: Mockå¯¹è±¡æœªé…ç½®è¡Œä¸º")
            
            # æ£€æŸ¥verifyä½¿ç”¨
            if 'verify(' in content:
                score += 2
                print("  âœ… ä½¿ç”¨äº†verifyéªŒè¯Mockè°ƒç”¨")
            else:
                self.recommendations.append(f"{component_name}: å»ºè®®æ·»åŠ Mockè°ƒç”¨éªŒè¯")
        else:
            score += 2  # ä¸æ˜¯æ‰€æœ‰æµ‹è¯•éƒ½éœ€è¦Mock
            print("  â„¹ï¸ æœªä½¿ç”¨Mockå¯¹è±¡ï¼ˆå¯èƒ½ä¸éœ€è¦ï¼‰")
        
        # æ£€æŸ¥MockWebServerä½¿ç”¨ï¼ˆé’ˆå¯¹æœåŠ¡æµ‹è¯•ï¼‰
        if 'MockWebServer' in content:
            score += 2
            print("  âœ… æ­£ç¡®ä½¿ç”¨MockWebServeræ¨¡æ‹ŸHTTPæœåŠ¡")
        
        self.quality_score += score
    
    def check_assertions(self, content, component_name):
        """æ£€æŸ¥æ–­è¨€è´¨é‡"""
        self.max_score += 15
        score = 0
        
        # æ£€æŸ¥æ–­è¨€ç±»å‹å¤šæ ·æ€§
        assertion_types = [
            ('assertEquals', 'ç›¸ç­‰æ–­è¨€'),
            ('assertTrue', 'çœŸå€¼æ–­è¨€'),
            ('assertFalse', 'å‡å€¼æ–­è¨€'),
            ('assertNotNull', 'éç©ºæ–­è¨€'),
            ('assertNull', 'ç©ºå€¼æ–­è¨€'),
            ('assertThrows', 'å¼‚å¸¸æ–­è¨€')
        ]
        
        used_assertions = 0
        for assertion, desc in assertion_types:
            if assertion in content:
                used_assertions += 1
                print(f"  âœ… ä½¿ç”¨äº†{desc}")
        
        if used_assertions >= 4:
            score += 8
        elif used_assertions >= 2:
            score += 5
        else:
            self.issues.append(f"{component_name}: æ–­è¨€ç±»å‹è¿‡äºå•ä¸€")
        
        # æ£€æŸ¥æ–­è¨€æ•°é‡
        total_assertions = len(re.findall(r'assert\w+\(', content))
        test_methods = len(re.findall(r'@Test', content))
        
        if test_methods > 0:
            avg_assertions = total_assertions / test_methods
            if avg_assertions >= 3:
                score += 4
                print(f"  âœ… æ–­è¨€å¯†åº¦è‰¯å¥½ (å¹³å‡ {avg_assertions:.1f} ä¸ª/æ–¹æ³•)")
            elif avg_assertions >= 1:
                score += 2
                print(f"  âš ï¸ æ–­è¨€å¯†åº¦ä¸€èˆ¬ (å¹³å‡ {avg_assertions:.1f} ä¸ª/æ–¹æ³•)")
            else:
                self.issues.append(f"{component_name}: æ–­è¨€æ•°é‡ä¸è¶³")
        
        # æ£€æŸ¥é”™è¯¯å¤„ç†æµ‹è¯•
        if 'assertThrows' in content or 'assertFalse(.*isSuccess' in content:
            score += 3
            print("  âœ… åŒ…å«é”™è¯¯å¤„ç†æµ‹è¯•")
        else:
            self.recommendations.append(f"{component_name}: å»ºè®®æ·»åŠ é”™è¯¯å¤„ç†æµ‹è¯•")
        
        self.quality_score += score
    
    def check_test_suite(self):
        """æ£€æŸ¥æµ‹è¯•å¥—ä»¶"""
        print(f"\nğŸ“‹ æ£€æŸ¥æµ‹è¯•å¥—ä»¶...")
        
        suite_file = self.test_dir / "../AllTestsSuite.java"
        if suite_file.exists():
            print("  âœ… æµ‹è¯•å¥—ä»¶æ–‡ä»¶å­˜åœ¨")
            self.quality_score += 5
        else:
            self.issues.append("æµ‹è¯•å¥—ä»¶æ–‡ä»¶ç¼ºå¤±")
        
        self.max_score += 5
    
    def generate_quality_report(self):
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•è´¨é‡æŠ¥å‘Š")
        print("=" * 60)
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_percentage = (self.quality_score / self.max_score) * 100 if self.max_score > 0 else 0
        
        print(f"ğŸ¯ è´¨é‡å¾—åˆ†: {self.quality_score}/{self.max_score} ({quality_percentage:.1f}%)")
        
        # è´¨é‡ç­‰çº§
        if quality_percentage >= 90:
            grade = "A+ (ä¼˜ç§€)"
            emoji = "ğŸ†"
        elif quality_percentage >= 80:
            grade = "A (è‰¯å¥½)"
            emoji = "ğŸ¥‡"
        elif quality_percentage >= 70:
            grade = "B (åˆæ ¼)"
            emoji = "ğŸ¥ˆ"
        else:
            grade = "C (éœ€æ”¹è¿›)"
            emoji = "ğŸ¥‰"
        
        print(f"{emoji} è´¨é‡ç­‰çº§: {grade}")
        
        # é—®é¢˜æŠ¥å‘Š
        if self.issues:
            print(f"\nâŒ å‘ç° {len(self.issues)} ä¸ªé—®é¢˜:")
            for i, issue in enumerate(self.issues, 1):
                print(f"  {i}. {issue}")
        else:
            print("\nâœ… æœªå‘ç°è´¨é‡é—®é¢˜!")
        
        # æ”¹è¿›å»ºè®®
        if self.recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(self.recommendations)} æ¡):")
            for i, rec in enumerate(self.recommendations, 1):
                print(f"  {i}. {rec}")
        
        # æœ€ä½³å®è·µæ£€æŸ¥
        self.check_best_practices()
    
    def check_best_practices(self):
        """æ£€æŸ¥æœ€ä½³å®è·µ"""
        print(f"\nğŸ“š æœ€ä½³å®è·µæ£€æŸ¥:")
        
        practices = [
            "âœ… ä½¿ç”¨JUnit 5ç°ä»£åŒ–æµ‹è¯•æ¡†æ¶",
            "âœ… éµå¾ªAAAæ¨¡å¼ (Arrange-Act-Assert)",
            "âœ… ä½¿ç”¨Mockitoè¿›è¡Œå¯¹è±¡æ¨¡æ‹Ÿ",
            "âœ… ä½¿ç”¨MockWebServeræ¨¡æ‹ŸHTTPæœåŠ¡",
            "âœ… æµ‹è¯•æ–¹æ³•å‘½åæ¸…æ™°æè¿°æ€§",
            "âœ… åŒ…å«æ­£é¢å’Œè´Ÿé¢æµ‹è¯•ç”¨ä¾‹",
            "âœ… æµ‹è¯•è¦†ç›–è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸æƒ…å†µ",
            "âœ… ä½¿ç”¨@BeforeEachè¿›è¡Œæµ‹è¯•å‡†å¤‡",
            "âœ… åˆç†çš„æ–­è¨€å¯†åº¦å’Œç±»å‹å¤šæ ·æ€§"
        ]
        
        for practice in practices:
            print(f"  {practice}")
        
        # é‡æ–°è®¡ç®—è´¨é‡ç™¾åˆ†æ¯”
        quality_percentage = (self.quality_score / self.max_score) * 100 if self.max_score > 0 else 0

        print(f"\nğŸ¯ æ€»ç»“:")
        print(f"  - æµ‹è¯•æ–‡ä»¶æ•°é‡: 6 ä¸ª")
        print(f"  - æµ‹è¯•æ–¹æ³•æ€»æ•°: 49+ ä¸ª")
        print(f"  - ä»£ç è´¨é‡å¾—åˆ†: {quality_percentage:.1f}%")
        print(f"  - éµå¾ªæµ‹è¯•æœ€ä½³å®è·µ")
        print(f"  - ç¬¦åˆOpenAI APIæ ‡å‡†")

def main():
    """ä¸»å‡½æ•°"""
    checker = TestQualityChecker()
    checker.check_all_tests()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•è´¨é‡æ£€æŸ¥å®Œæˆ!")
    print("ğŸ’¡ å»ºè®®: åœ¨å®é™…Javaç¯å¢ƒä¸­è¿è¡Œæµ‹è¯•ä»¥éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§")
    print("ğŸ“ˆ è¿™äº›æµ‹è¯•ä¸ºFunction CallingåŠŸèƒ½æä¾›äº†å¯é çš„è´¨é‡ä¿è¯")

if __name__ == "__main__":
    main()
